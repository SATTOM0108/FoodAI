{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important details\n",
    "##### Set shuffle equals true or model cant seem to learn (local maxima)\n",
    "\n",
    "###### Key details\n",
    "1. Tensorflow 1.9.0\n",
    "2. Keras 2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17321941593608493447\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11270533940\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4626270998032556798\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n",
      "Found 276675 images belonging to 406 classes.\n",
      "Found 34471 images belonging to 406 classes.\n",
      "Found 34478 images belonging to 406 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, AveragePooling2D\n",
    "from keras import applications\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam  \n",
    "import keras,os,re,math,itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#import cv2\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "\n",
    "#check GPU usage\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 299, 299\n",
    "\n",
    "#important PATH variables\n",
    "rootPath = r'/notebooks/storage'\n",
    "full_model_weights_path = os.path.join(rootPath,'weights','full_model_weights.h5')\n",
    "trainUrl = os.path.join(rootPath, 'data', 'Train')\n",
    "testUrl =  os.path.join(rootPath, 'data', 'Test')\n",
    "valUrl =  os.path.join(rootPath, 'data', 'Val')\n",
    "class_indices_path = os.path.join(rootPath,'class_indices.npy')\n",
    "modelCheckpointPath = os.path.join(rootPath,'models','model4b.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "csvLoggerPath = os.path.join(rootPath,'log','model4b.log')\n",
    "            \n",
    "\n",
    "# number of epochs to train top model\n",
    "epochs = 20\n",
    "\n",
    "# batch size used by flow_from_directory and predict_generator, try 128 if possible\n",
    "batch_size = 64\n",
    "#important hyper-parameters\n",
    "metrics = ['accuracy']\n",
    "\n",
    "datagenTrain = ImageDataGenerator(\n",
    "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False, # randomly flip images\n",
    "    zoom_range=[.8, 1],\n",
    "    channel_shift_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range = 0.2,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function = preprocess_input) #preprocess_input is the preprocessing function used in \n",
    "                                                #original inceptionV3 model\n",
    "\n",
    "generatorTrain = datagenTrain.flow_from_directory(\n",
    "    trainUrl,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "#repeat the prediction for validation dataset\n",
    "datagenVal = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "generatorVal = datagenVal.flow_from_directory(\n",
    "    valUrl,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle= True)\n",
    "\n",
    "\n",
    "\n",
    "#repeat the prediction for test dataset\n",
    "datagenTest = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "generatorTest = datagenVal.flow_from_directory(\n",
    "    testUrl,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle= True)\n",
    "\n",
    "#calculate some useful variables\n",
    "nb_train_samples = len(generatorTrain.filenames)\n",
    "num_classes = len(generatorTrain.class_indices)\n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "np.save(class_indices_path, generatorTrain.class_indices)    # save the class indices to use use later in predictions\n",
    "    \n",
    "#predict the number of steps required for batch size\n",
    "nb_validation_samples = len(generatorVal.filenames)\n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def show_images(unprocess=True):\n",
    "#used to visualise images \n",
    "    plt.clf()\n",
    "    def reverse_preprocess_input(x0):\n",
    "        x = x0 / 2.0\n",
    "        x += 0.5\n",
    "        x *= 255.\n",
    "        return x\n",
    "    #import pdb;pdb.set_trace()\n",
    "    for x in generatorTrain:\n",
    "        fig, axes = plt.subplots(nrows=8, ncols=4)\n",
    "        fig.set_size_inches(8, 8)\n",
    "        page = 0\n",
    "        page_size = 32\n",
    "        start_i = page * page_size\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            img = x[0][i+start_i]\n",
    "            \n",
    "            if unprocess:\n",
    "                im = ax.imshow(reverse_preprocess_input(img).astype('uint8') )\n",
    "            else:\n",
    "                im = ax.imshow((img/2.0)+.5) #matplotlib can only plot between range of 0 and 1\n",
    "                \n",
    "            ax.set_axis_off()\n",
    "            ax.title.set_visible(False)\n",
    "            ax.xaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticks([])\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "        plt.subplots_adjust(left=0, wspace=0, hspace=0)\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "def _build_model():\n",
    "    #import pdb;pdb.set_trace()\n",
    "    K.clear_session()\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))\n",
    "    x = base_model.output\n",
    "    x = AveragePooling2D(pool_size=(8, 8))(x)\n",
    "    x = Dropout(.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    predictions = Dense(num_classes, init='glorot_uniform', W_regularizer=l2(.0005), activation='softmax')(x)\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    return model\n",
    "\n",
    "def _load_model(pathName= None):\n",
    "    \n",
    "    if pathName != None:\n",
    "        print('loading model from {0}'.format(pathName))\n",
    "        return keras.models.load_model(pathName)\n",
    "    files  = os.listdir(os.path.join(rootPath,'models'))\n",
    "    files.remove('.ipynb_checkpoints')\n",
    "    lowest_val_loss = None\n",
    "    model_path = None\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print('There is no save models. Going to create a new model now')\n",
    "        final_model = _build_model()\n",
    "        return final_model\n",
    "    \n",
    "    #loop through all files and find the path of the file with lowest loss\n",
    "    for file in files:\n",
    "        file_val_loss = re.findall(r'\\d\\.\\d{2}',file) #regex expression to extract the val loss\n",
    "        if lowest_val_loss  == None:\n",
    "            lowest_val_loss = file_val_loss\n",
    "            model_path = file\n",
    "        if file_val_loss < lowest_val_loss:\n",
    "            lowest_val_loss = file_val_loss\n",
    "            model_path = file\n",
    "    print('Model loaded from path:{0}'.format(model_path))\n",
    "    \n",
    "    return keras.models.load_model(os.path.join(rootPath,'models',model_path))\n",
    "\n",
    "\n",
    "def plot_graphs(history):\n",
    "    # A history object is a keras object used to store accuracy and loss\n",
    "    \n",
    "    plt.figure(1)\n",
    "    # summarize history for accuracy\n",
    "    plt.subplot(211)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.subplot(212)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def train_top_model(pathName= None):\n",
    "    #we are only training the top layer of the model and not the whole model\n",
    "    model = _load_model(pathName)\n",
    "    for layer in model.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "    #import pdb;pdb.set_trace()\n",
    "    adam =Adam(lr=0.0001)\n",
    "    model.compile(optimizer= adam ,loss='categorical_crossentropy', metrics=metrics)\n",
    "    history = model.fit_generator(generatorTrain,\n",
    "                    validation_data=generatorVal,\n",
    "                    steps_per_epoch=predict_size_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1, max_queue_size=15, workers = 4, use_multiprocessing = True)\n",
    "    model.save(os.path.join(rootPath,'models','model4b.00-9.99hdf5'))\n",
    "    print('Model saved (pretrain model)')\n",
    "    return history, model\n",
    "\n",
    "def train_whole_model(pathName= None):\n",
    "    \n",
    "    #\n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "    model = _load_model(pathName)\n",
    "    \n",
    "    \n",
    "    #only uncomment this code if training model from saved model which is only trained from train_top_model\n",
    "    #for layer in model.layers:\n",
    "    #    layer.trainable = True\n",
    "    #model.compile(optimizer= 'adam',loss='categorical_crossentropy', metrics=metrics)\n",
    "    \n",
    "    csv_logger = CSVLogger(csvLoggerPath)\n",
    "    checkpointer = ModelCheckpoint(filepath=modelCheckpointPath, verbose=1, save_best_only=True)   \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.0001,verbose =1)\n",
    "    history = model.fit_generator(generatorTrain,\n",
    "                    validation_data=generatorVal,\n",
    "                    steps_per_epoch=predict_size_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=([csv_logger, checkpointer,reduce_lr]),\n",
    "                    max_queue_size=10, workers = 8, \n",
    "                    use_multiprocessing = True)                \n",
    "    return history, model\n",
    "\n",
    "def model_evaluate(model):\n",
    "    (eval_loss, eval_accuracy) = model.evaluate_generator(generatorTest,steps=None,verbose=1)\n",
    "    print('Evaluating the:'.format(model.metrics_names))\n",
    "\n",
    "    print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\n",
    "    print(\"[INFO] Loss: {}\".format(eval_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "### Training is split into 2 phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualise some images to explore data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase one: Train top model\n",
    "##### Not that effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train top model\n",
    "history,model = train_top_model()\n",
    "\n",
    "plot_graphs(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase two: Train the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train whole model\n",
    "final_history, final_model = train_whole_model()\n",
    "plot_graphs(final_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = _load_model()\n",
    "model_evaluate(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,imgPath,topNum = 5):\n",
    "    # load the class_indices saved in the earlier step\n",
    "    class_dictionary = np.load('class_indices.npy').item()\n",
    "    num_classes = len(class_dictionary)\n",
    "    # add the path to your test image below\n",
    "    orig = cv2.imread(imagePath)\n",
    "\n",
    "    print(\"[INFO] loading and preprocessing image...\")\n",
    "    image = load_img(imagePath, target_size=(299, 299))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    #do all the preprocessing for the images\n",
    "    # important! otherwise the predictions will be '0'\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    #getting the predictions\n",
    "    y_pred_prob = model.predict(image)\n",
    "    class_predicted_ix = np.argmax(y_pred_prob, axis=1)\n",
    "    top_n_preds_ix = np.argpartition(y_pred_prob, -topNum)[:,-topNum:]\n",
    "\n",
    "    #convert from ix to class label\n",
    "    inv_map = {v: k for k, v in class_dictionary.items()}\n",
    "    label = inv_map[inID]\n",
    "\n",
    "    # get the prediction label\n",
    "    print(\"Image ID: {}, Label: {}\".format(class_predicted_ix[0], label))\n",
    "\n",
    "    # display the predictions with the image\n",
    "    cv2.putText(orig, \"Predicted: {}\".format(label), (10, 30),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 1.5, (43, 99, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Classification\", orig)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from path:model4b.05-1.27.hdf5\n",
      "Found 34478 images belonging to 406 classes.\n",
      "539/539 [==============================] - 392s 727ms/step\n"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(model,generator):\n",
    "    y_pred = model.predict_generator(generator,verbose=1)\n",
    "    y_pred_value = np.argmax(y_pred, axis=1)\n",
    "    conf = confusion_matrix(generatorTest.classes,y_pred_value)\n",
    "    \n",
    "    #find the largest n indices in a multi dimensional array\n",
    "    def largest_indices(ary, n):\n",
    "        \"\"\"Returns the n largest indices from a numpy array.\"\"\"\n",
    "        flat = ary.flatten()\n",
    "        indices = np.argpartition(flat, -n)[-n:]\n",
    "        indices = indices[np.argsort(-flat[indices])]\n",
    "        return np.unravel_index(indices, ary.shape)\n",
    "    \n",
    "    class_dictionary = np.load('/storage/class_indices.npy').item()\n",
    "    inv_map = {v: k for k, v in class_dictionary.items()}\n",
    "    row_ix ,col_ix = largest_indices(conf,50)\n",
    "\n",
    "    #change from index value to labels\n",
    "    row = [inv_map[x] for x in row_ix]\n",
    "    col = [inv_map[x] for x in col_ix]\n",
    "\n",
    "    return conf,list(zip(row,col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
